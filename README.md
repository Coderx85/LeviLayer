# LeviLayer 
## Activation Function Analysis

<!-- ![LeviLayer Activation Function Analysis](https://your-image-url-here.com) -->

### Overview

This project provides an interactive dashboard for analyzing the LeviLayer activation function in neural networks. LeviLayer is a novel activation function that has shown promising results in various deep learning tasks. With this dashboard, users can explore the behavior of LeviLayer and compare it with other popular activation functions.

### Features

- **Parameter Tuning**: Adjust learning rate, number of neurons, and number of data points for analysis.
- **Model Comparison**: Compare LeviLayer with ReLU, Sigmoid, Tanh, Leaky ReLU, and ELU activation functions.
- **Data Visualization**: Visualize the LeviLayer activation function and compare it with other functions using interactive plots.

### Installation

To run this project locally, follow these steps:

1. Clone this repository:

   ```bash
   git clone https://github.com/Priyanshu085/levilayer.git
   ```

2. Navigate to the project directory:

   ```bash
   cd levilayer
   ```

3. Install the required dependencies:

   ```bash
   pip install -r requirements.txt
   ```

4. Run the Streamlit app:

   ```bash
   streamlit run index.py
   ```

5. Access the app in your browser at `http://localhost:8501`.

### Usage

- Adjust the parameters in the sidebar to tune LeviLayer and compare it with other activation functions.
- Click the "Visualize" button to generate interactive plots of the activation functions.
- Explore different settings and observe how LeviLayer behaves under various conditions.

### License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

### Credits

This project was created and maintained by [Priyanshu](https://github.com/Priyanshu085).